{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import glob2 as glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper \n",
    "from src.Helper.merger import MERGER\n",
    "from src.Helper.filters import ApplyPreProccessingTask\n",
    "\n",
    "# Data Pre-Processing\n",
    "from src.DataPreProcessing.DataCleaning import CLEANER\n",
    "from src.DataPreProcessing.DataHandleMissingValues import HandleMissingValues\n",
    "from src.DataPreProcessing.DataEncodingCategoricalVariables import EncodingCategorical\n",
    "from src.DataPreProcessing.DataFeatureScaling import FeatureScaler\n",
    "from src.DataPreProcessing.DataFeatureEngineering import FeatureEngineering\n",
    "from src.DataPreProcessing.BalancingDataset import DatasetBalancer\n",
    "from src.DataPreProcessing.DataHandlingOutlier import OutlierDetector\n",
    "from src.DataPreProcessing.DataFeatureSelection import FeatureSelector\n",
    "\n",
    "# Exploratory Data Analysis(EDA)\n",
    "from src.EDA.eda import EDA\n",
    "\n",
    "# Hypothesis Tests\n",
    "from src.HypothesisTesting.HypothesisTests import HypothesisTests\n",
    "from src.HypothesisTesting.ValidateTests import ValidateHypothesisTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Clear existing handlers\n",
    "logger.handlers = []\n",
    "\n",
    "# File handler\n",
    "file_handler = logging.FileHandler(\"logs.txt\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Disable propagation to avoid duplicate logs from parent loggers\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 10:48:28,076 - __main__ - INFO - Searching for all csv files in Raw foler.\n",
      "2024-11-18 10:48:28,078 - __main__ - INFO - RAW FILE 01 ./src/Data/Raw\\heart_01.csv\n",
      "2024-11-18 10:48:28,078 - __main__ - INFO - RAW FILE 02 ./src/Data/Raw\\heart_02.csv\n",
      "2024-11-18 10:48:28,080 - __main__ - INFO - RAW FILE 03 ./src/Data/Raw\\heart_03.csv\n",
      "2024-11-18 10:48:28,081 - __main__ - INFO - RAW FILE 04 ./src/Data/Raw\\heart_04.csv\n"
     ]
    }
   ],
   "source": [
    "info = f\"Searching for all csv files in Raw foler.\"\n",
    "logger.info(info)\n",
    "raw_csv_files = glob.glob(\"./src/Data/Raw/*.csv\")\n",
    "\n",
    "for index, file in enumerate(raw_csv_files):\n",
    "    info = f\"RAW FILE {str(index + 1).zfill(2)} {file}\"\n",
    "    logger.info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 10:48:28,860 - __main__ - INFO - Loading all csv files.\n",
      "2024-11-18 10:48:28,862 - __main__ - INFO - Loading file ./src/Data/Raw\\heart_01.csv to merger.\n",
      "2024-11-18 10:48:28,874 - __main__ - INFO - [MERGER] Successfully loaded file: ./src/Data/Raw\\heart_01.csv\n",
      "2024-11-18 10:48:28,876 - __main__ - INFO - File ./src/Data/Raw\\heart_01.csv to loaded successfully in MERGER.\n",
      "2024-11-18 10:48:28,877 - __main__ - INFO - Loading file ./src/Data/Raw\\heart_02.csv to merger.\n",
      "2024-11-18 10:48:28,893 - __main__ - INFO - [MERGER] Successfully loaded file: ./src/Data/Raw\\heart_02.csv\n",
      "2024-11-18 10:48:28,894 - __main__ - INFO - File ./src/Data/Raw\\heart_02.csv to loaded successfully in MERGER.\n",
      "2024-11-18 10:48:28,895 - __main__ - INFO - Loading file ./src/Data/Raw\\heart_03.csv to merger.\n",
      "2024-11-18 10:48:28,909 - __main__ - INFO - [MERGER] Successfully loaded file: ./src/Data/Raw\\heart_03.csv\n",
      "2024-11-18 10:48:28,910 - __main__ - INFO - File ./src/Data/Raw\\heart_03.csv to loaded successfully in MERGER.\n",
      "2024-11-18 10:48:28,911 - __main__ - INFO - Loading file ./src/Data/Raw\\heart_04.csv to merger.\n",
      "2024-11-18 10:48:28,939 - __main__ - INFO - [MERGER] Successfully loaded file: ./src/Data/Raw\\heart_04.csv\n",
      "2024-11-18 10:48:28,940 - __main__ - INFO - File ./src/Data/Raw\\heart_04.csv to loaded successfully in MERGER.\n",
      "2024-11-18 10:48:28,941 - __main__ - INFO - Saving file to u:\\Projects\\[2922] Data Processing\\src/Data/Merged/merged.csv\n",
      "2024-11-18 10:48:29,026 - __main__ - INFO - Data successfully saved to u:\\Projects\\[2922] Data Processing\\src/Data/Merged/merged.csv\n"
     ]
    }
   ],
   "source": [
    "info = f\"Loading all csv files.\"\n",
    "logger.info(info)\n",
    "merger = MERGER(logger=logger)\n",
    "\n",
    "for index, file in enumerate(raw_csv_files):\n",
    "    info = f\"Loading file {file} to merger.\"\n",
    "    logger.info(info)\n",
    "    response = merger.load_csv_file(file)\n",
    "    \n",
    "    if response[\"success\"]:\n",
    "        info = f\"File {file} to loaded successfully in MERGER.\"\n",
    "        logger.info(info)\n",
    "    else:\n",
    "        info = f\"Unable to load File-{file} in MERGER. Skipping...\"\n",
    "        logger.warning(info)\n",
    "\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Merged/merged.csv\")\n",
    "info = f\"Saving file to {output_file}\"\n",
    "logger.info(info)\n",
    "response = merger.save_merged_data(output_file=output_file)\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLEANER\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Cleaned/cleaned.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Cleaning\", \n",
    "    task_class = CLEANER, \n",
    "    sub_func = \"clean_data\", \n",
    "    sub_func_args = None, \n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HandleMissingValues\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/handlemissingvalues.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Missing Values\", \n",
    "    task_class = HandleMissingValues, \n",
    "    sub_func = \"handle_missing\", \n",
    "    sub_func_args = {\n",
    "        'strategy': {\n",
    "            'age': 'median',\n",
    "            'trestbps': 'mean',\n",
    "            'chol': 'mean',\n",
    "            'thalach': 'mean',\n",
    "            'oldpeak': 'mean',\n",
    "            'cp': 'most_frequent',\n",
    "            'restecg': 'most_frequent',\n",
    "            'slope': 'most_frequent',\n",
    "            'ca': 'most_frequent',\n",
    "            'thal': 'most_frequent'\n",
    "        },\n",
    "        'threshold': 0.5  # Drop if more than 50% values are missing\n",
    "    },\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply EncodingCategorical\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/processedencodeddata.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Encoding Categorical\", \n",
    "    task_class = EncodingCategorical, \n",
    "    sub_func = \"encode_categorical\", \n",
    "    sub_func_args = {\n",
    "        'label_encode': ['cp', 'restecg'],\n",
    "        'onehot_encode': ['slope', 'ca', 'thal'],\n",
    "        'drop_first': True\n",
    "    },\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaler Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FeatureScaler handler\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/scaled_data.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Feature Scaler Handler\", \n",
    "    task_class = FeatureScaler, \n",
    "    sub_func = \"fit_transform\", \n",
    "    sub_func_args = {\n",
    "        'method' : \"normalize\"\n",
    "    },\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Feature Engineering handler\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/engineered_data.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Feature Engineering Handler\", \n",
    "    task_class = FeatureEngineering, \n",
    "    sub_func = \"encode_categorical_features\", \n",
    "    sub_func_args = {\n",
    "        'encoding_method': \"one_hot\"\n",
    "    },\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Dataset Balancer\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/balanced_data.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Dataset Balancer\", \n",
    "    task_class = DatasetBalancer, \n",
    "    sub_func = \"balance_data\", \n",
    "    sub_func_args = {\n",
    "        'method': \"smote\"\n",
    "    },\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detector & Removal Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Outlier Detector handler\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/outliers_removed.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Outlier Detector & Removal Handler\", \n",
    "    task_class = OutlierDetector, \n",
    "    sub_func = \"remove_outliers\", \n",
    "    sub_func_args = None,\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selector Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Outlier Detector handler\n",
    "output_file = os.path.join(BASE_DIR, \"src/Data/Processed/selected_features.csv\")\n",
    "\n",
    "ApplyPreProccessingTask(\n",
    "    task = \"Feature Selector Handler\", \n",
    "    task_class = FeatureSelector, \n",
    "    sub_func = \"select_k_best\", \n",
    "    sub_func_args = {\n",
    "        'target': \"target\", \n",
    "        'k': 5\n",
    "    },\n",
    "    input_file = file, \n",
    "    output_file = output_file, \n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis(EDA)\n",
    "eda = EDA(logger=logger)\n",
    "graphs_dir = os.path.join(BASE_DIR, \"src/Graphs/\")\n",
    "eda.set_graphs_dir(graphs_dir)\n",
    "eda.load_data(file)\n",
    "eda.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uploaded dataset\n",
    "file_path = './src/Data/Merged/merged.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and summary info\n",
    "data.head(), data.info()\n",
    "\n",
    "# Setting the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust the plot to avoid issues with mismatched tick labels\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(\"Exploratory Data Analysis for Heart Disease Prediction Dataset\")\n",
    "\n",
    "# Plot 1: Distribution of 'age' and 'target' (presence of heart disease)\n",
    "sns.histplot(data, x='age', hue='target', multiple='stack', bins=20, palette='viridis', ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Age Distribution by Heart Disease Status\")\n",
    "\n",
    "# Plot 2: Heart disease prevalence by sex\n",
    "sns.countplot(data=data, x='sex', hue='target', palette='viridis', ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Heart Disease Status by Sex\")\n",
    "\n",
    "# Plot 3: Heart disease prevalence by chest pain type\n",
    "sns.countplot(data=data, x='cp', hue='target', palette='viridis', ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Heart Disease Status by Chest Pain Type\")\n",
    "\n",
    "# Plot 4: Correlation heatmap for numerical features\n",
    "corr_matrix = data[['age', 'trestbps', 'chol', 'fbs', 'thalach', 'oldpeak', 'ca', 'thal', 'target']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Correlation Matrix for Numerical Features\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hypothesis_tests = HypothesisTests(logger=logger)\n",
    "    response = hypothesis_tests.load_data(file=file)\n",
    "    response = hypothesis_tests.run_all_tests()\n",
    "    \n",
    "    if response[\"success\"]:\n",
    "        validate_hypothesis_tests = ValidateHypothesisTesting(logger=logger, hypothesis_tests=hypothesis_tests)\n",
    "        response = validate_hypothesis_tests.run_all_tests()\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occured: Error: {e}\")\n",
    "    logger.error(traceback.print_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
